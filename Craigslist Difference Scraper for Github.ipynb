{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Reqired Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set base_path where previously scraped information and pictures are saved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.normpath(r'C:\\path\\to\\previously\\saved\\scraped\\files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the most recent file written in the directory.  Save its timestamp into folder_time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(base_path, '*')\n",
    "files = sorted(glob.iglob(files_path), key=os.path.getctime, reverse=True)\n",
    "timestamp = seconds = os.path.getctime(files[0])\n",
    "folder_time = datetime.fromtimestamp(timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the base urls you're using.  List all of the Craigslist sites you want to scrape in search_urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "craigslist_url = 'https://cincinnati.craigslist.org'\n",
    "base_url = 'https://cincinnati.craigslist.org/search/jwa'\n",
    "\n",
    "search_urls = ['https://akroncanton.craigslist.org/d/jewelry/search/jwa', 'https://ashtabula.craigslist.org/d/jewelry/search/jwa', 'https://athensohio.craigslist.org/d/jewelry/search/jwa', 'https://chillicothe.craigslist.org/d/jewelry/search/jwa', 'https://cincinnati.craigslist.org/d/jewelry/search/jwa', 'https://cleveland.craigslist.org/d/jewelry/search/jwa', 'https://columbus.craigslist.org/d/jewelry/search/jwa', 'https://dayton.craigslist.org/d/jewelry/search/jwa', 'https://limaohio.craigslist.org/d/jewelry/search/jwa', 'https://mansfield.craigslist.org/d/jewelry/search/jwa', 'https://sandusky.craigslist.org/d/jewelry/search/jwa','https://toledo.craigslist.org/d/jewelry/search/jwa', 'https://tuscarawas.craigslist.org/d/jewelry/search/jwa', 'https://youngstown.craigslist.org/d/jewelry/search/jwa', 'https://zanesville.craigslist.org/d/jewelry/search/jwa', 'https://bgky.craigslist.org/d/jewelry/search/jwa', 'https://eastky.craigslist.org/d/jewelry/search/jwa', 'https://lexington.craigslist.org/d/jewelry/search/jwa', 'https://louisville.craigslist.org/d/jewelry/search/jwa', 'https://owensboro.craigslist.org/d/jewelry/search/jwa', 'https://westky.craigslist.org/d/jewelry/search/jwa', 'https://bloomington.craigslist.org/d/jewelry/search/jwa', 'https://evansville.craigslist.org/d/jewelry/search/jwa', 'https://fortwayne.craigslist.org/d/jewelry/search/jwa', 'https://indianapolis.craigslist.org/d/jewelry/search/jwa', 'https://kokomo.craigslist.org/d/jewelry/search/jwa', 'https://tippecanoe.craigslist.org/d/jewelry/search/jwa', 'https://muncie.craigslist.org/d/jewelry/search/jwa', 'https://richmondin.craigslist.org/d/jewelry/search/jwa', 'https://southbend.craigslist.org/d/jewelry/search/jwa', 'https://terrehaute.craigslist.org/d/jewelry/search/jwa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_anchors = []\n",
    "first_page_anchors = []\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/12.0'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the post urls from every craigslist site listed in search_urls.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_url in search_urls:\n",
    "\n",
    "    response = get(search_url, headers=headers)\n",
    "\n",
    "    search_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    first_page_anchors =  search_soup.find_all(\"a\", {\"class\":\"result-title hdrlnk\"})\n",
    "    for anchor in first_page_anchors:\n",
    "        item_anchors.append(anchor)\n",
    "\n",
    "    while True:\n",
    "        if search_soup.find(\"a\",{\"class\":\"button next\"}) is None:\n",
    "            break\n",
    "        else:\n",
    "            if search_soup.find(\"a\",{\"class\":\"button next\"})['href'] != '':\n",
    "                response = get(craigslist_url + search_soup.find(\"a\",{\"class\":\"button next\"})['href'], headers=headers)\n",
    "                search_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                new_anchors = search_soup.find_all(\"a\", {\"class\":\"result-title hdrlnk\"})\n",
    "                for anchor in new_anchors:\n",
    "                    item_anchors.append(anchor)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicate post listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure not to scrape the same item twice\n",
    "cleaned_anchors = []\n",
    "for anchor in item_anchors:\n",
    "    if anchor['href'] not in cleaned_anchors:\n",
    "        cleaned_anchors.append(anchor['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the post title, item price, post location, date of the post, the item image url, and the post url of posts that were posted after the last scraping took place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for link in cleaned_anchors:\n",
    "    item_response = get(link, headers = headers)\n",
    "    item_soup = BeautifulSoup(item_response.text,'html.parser')\n",
    "\n",
    "    if item_soup.find(\"time\", {\"class\":\"date timeago\"}) is not None:\n",
    "        item_raw_date = item_soup.find(\"time\", {\"class\":\"date timeago\"})['datetime']\n",
    "\n",
    "        date_list = item_raw_date.split('-')\n",
    "        day_time = date_list[2].split('T')\n",
    "        time_list = day_time[1].split(':')\n",
    "\n",
    "        file_time = datetime(year=int(date_list[0]), month=int(date_list[1]), day=int(day_time[0]), hour=int(time_list[0]), minute=int(time_list[1]), second=int(time_list[2] ) )\n",
    "\n",
    "        if folder_time < file_time:\n",
    "            item_parse_date = item_raw_date[:-2] + \":\" + item_raw_date[-2:]\n",
    "            d = datetime.fromisoformat(item_parse_date)\n",
    "\n",
    "            o = urlparse(link)\n",
    "            location = o.netloc.split('.')[0]\n",
    "\n",
    "            item_title = item_soup.find(\"span\", {\"id\":\"titletextonly\"}).get_text()\n",
    "            cleaner_title = re.sub(r'[<>:\"\\/\\\\|?*]','',item_title)\n",
    "            clean_title = re.sub(r'\\t',' ', cleaner_title)\n",
    "\n",
    "            if item_soup.find(\"span\", {\"class\":\"price\"}) is not None:\n",
    "                item_price = item_soup.find(\"span\", {\"class\":\"price\"}).get_text()\n",
    "            else:\n",
    "                item_price = \"not listed\"\n",
    "            if item_soup.find(\"div\", {\"class\": \"slide first visible\"}) is not None:\n",
    "                item_image_url = item_soup.find(\"div\", {\"class\": \"slide first visible\"}).img['src']\n",
    "            else:\n",
    "                item_image_url = \"no image\"\n",
    "            item_raw_date = item_soup.find(\"time\", {\"class\":\"date timeago\"})['datetime']\n",
    "            item_parse_date = item_raw_date[:-2] + \":\" + item_raw_date[-2:]\n",
    "            d = datetime.fromisoformat(item_parse_date)\n",
    "\n",
    "\n",
    "            records.append([clean_title, item_price, location, d.strftime(\"%a %b %d %I:%M %p %Z\") , item_image_url, link])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new repository for post information and photos to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repository = os.path.normpath(r'C:\\path\\to\\new\\file')\n",
    "\n",
    "file_time = datetime.now().strftime(\"%b %d %Y %Hh%Mm%Ss\")\n",
    "\n",
    "new_file = os.mkdir(os.path.join(base_repository, file_time ))\n",
    "\n",
    "os.chdir(os.path.join(base_repository, file_time))\n",
    "\n",
    "new_repository = os.path.normpath(os.path.join(base_repository, file_time))\n",
    "\n",
    "#new_repository = os.path.normpath(os.path.join(base_repository, datetime.now().strftime(\"%b %d %Y %Hh%Mm%Ss\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save post information in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cragislist_update '+ datetime.now().strftime(\"%b %d %Y %Hh%Mm%Ss\") + '.csv', mode='w', newline='\\n', encoding=\"utf-8\") as output_file:\n",
    "    ghostwriter = csv.writer(output_file)\n",
    "    ghostwriter.writerow(['Item Title', 'item price', 'location', 'post date', 'image url', 'Post url'])\n",
    "    ghostwriter.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save images to new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = 0\n",
    "for record in records:\n",
    "    if record[4] != 'no image':\n",
    "        urllib.request.urlretrieve(record[4], os.path.join(new_repository, record[0] + '_' + str(line_count) + '.png') )\n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
